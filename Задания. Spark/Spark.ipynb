{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aseZA3tWG7AS",
        "HZGK4T2Z9qAi",
        "GW4jcniqLmjs",
        "VoONM_lVaIs1",
        "5eXnbdBJaTv0",
        "C7_Ub6csGWLy",
        "8Ov24g2eMIa_",
        "eUdrQfVXfQdc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Тестовые задачи по взаимодействию с SparkAPI"
      ],
      "metadata": {
        "id": "-ZqmwTR0TX7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установка спарка на машину"
      ],
      "metadata": {
        "id": "E5uUUTF9GaPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLGSHcgrtPSW",
        "outputId": "7aa806d4-6030-4a30-b492-83b75e2a6d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратимся к гиту, где лежат некоторые файлы для выполнения задач."
      ],
      "metadata": {
        "id": "Ww2KE71AmXh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/databricks/LearningSparkV2.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOnpATOXLhBW",
        "outputId": "ca7aa82c-43d1-45d0-973f-2985c77a5b1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LearningSparkV2'...\n",
            "remote: Enumerating objects: 1720, done.\u001b[K\n",
            "remote: Counting objects: 100% (1720/1720), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1036/1036), done.\u001b[K\n",
            "remote: Total 1720 (delta 546), reused 1691 (delta 541), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1720/1720), 76.97 MiB | 11.67 MiB/s, done.\n",
            "Resolving deltas: 100% (546/546), done.\n",
            "Updating files: 100% (768/768), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задачи"
      ],
      "metadata": {
        "id": "t4X4L5UM3HgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Возможности спарка"
      ],
      "metadata": {
        "id": "aseZA3tWG7AS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Собираем библиотеки\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, DateType, FloatType, IntegerType\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "#ссылка на загрузку CSV файла\n",
        "url = f'https://drive.google.com/uc?id=1ZuLZDoPboHnM1P8m3PflIe70zUGvjsRq'\n",
        "#Загружаем файл\n",
        "response = requests.get(url)\n",
        "#Проверяем статус ответа, если ок, то открываем файл car в памяти\n",
        "if response.status_code == 200:\n",
        "    with open('car.csv', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Файл загружен\")\n",
        "else:\n",
        "    print(f\"Файл не загрузился {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn_6t5Lttmcj",
        "outputId": "dfb70b2c-fadd-4846-8c68-b1516a390c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл загружен\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Делаем функцию, которая принимает загружаемый файл и открывает его как объект спарка\n",
        "\n",
        "#вытаскиваем имя выгружаемого файла\n",
        "file_name = str(f).split(\"name='\")[1].split(\"'\")[0]\n",
        "\n",
        "def create_spark_object(file_name):\n",
        "    if str(f).lower().find(file_name) > 1:\n",
        "        spark = SparkSession.builder.appName('Test').getOrCreate()\n",
        "        dt = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_name)\n",
        "        return dt\n",
        "df = create_spark_object(file_name)\n",
        "\n",
        "df.show(1, vertical = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytbxMlmKriHS",
        "outputId": "e018681a-ba89-4a7e-c5a3-4e1d8c52dcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0-------------------------\n",
            " manufacturer_name | Subaru       \n",
            " model_name        | Outback      \n",
            " transmission      | automatic    \n",
            " color             | silver       \n",
            " odometer_value    | 190000       \n",
            " year_produced     | 2010         \n",
            " engine_fuel       | gasoline     \n",
            " engine_has_gas    | False        \n",
            " engine_type       | gasoline     \n",
            " engine_capacity   | 2.5          \n",
            " body_type         | universal    \n",
            " has_warranty      | False        \n",
            " state             | owned        \n",
            " drivetrain        | all          \n",
            " price_usd         | 10900.0      \n",
            " is_exchangeable   | False        \n",
            " location_region   | Минская обл. \n",
            " number_of_photos  | 9            \n",
            " up_counter        | 13           \n",
            " feature_0         | False        \n",
            " feature_1         | True         \n",
            " feature_2         | True         \n",
            " feature_3         | True         \n",
            " feature_4         | False        \n",
            " feature_5         | True         \n",
            " feature_6         | False        \n",
            " feature_7         | True         \n",
            " feature_8         | True         \n",
            " feature_9         | True         \n",
            " duration_listed   | 16           \n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Select\n",
        "df.select(\"manufacturer_name\",\"model_name\").show(1)\n",
        "df.select(df[\"manufacturer_name\"],df[\"model_name\"]).show(1)\n",
        "df.select(F.col(\"manufacturer_name\"), F.col(\"model_name\")).show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av9Za4gMmE5v",
        "outputId": "37db2b3f-2e5a-4b61-f519-beadb172c404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------+\n",
            "|manufacturer_name|model_name|\n",
            "+-----------------+----------+\n",
            "|           Subaru|   Outback|\n",
            "+-----------------+----------+\n",
            "only showing top 1 row\n",
            "\n",
            "+-----------------+----------+\n",
            "|manufacturer_name|model_name|\n",
            "+-----------------+----------+\n",
            "|           Subaru|   Outback|\n",
            "+-----------------+----------+\n",
            "only showing top 1 row\n",
            "\n",
            "+-----------------+----------+\n",
            "|manufacturer_name|model_name|\n",
            "+-----------------+----------+\n",
            "|           Subaru|   Outback|\n",
            "|           Subaru|   Outback|\n",
            "|           Subaru|  Forester|\n",
            "+-----------------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter\n",
        "NAME = 'Audi'\n",
        "\n",
        "df\\\n",
        "   .select(\"manufacturer_name\",\"model_name\",\"transmission\", \"color\")\\\n",
        "   .filter(F.col(\"manufacturer_name\") == NAME)\\\n",
        "   .filter(\"transmission = 'automatic' and color = 'black'\").show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkwbbpypnN-f",
        "outputId": "34acaca6-8f14-40ef-cfab-f7f25f57903d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------+------------+-----+\n",
            "|manufacturer_name|model_name|transmission|color|\n",
            "+-----------------+----------+------------+-----+\n",
            "|             Audi|        TT|   automatic|black|\n",
            "|             Audi|        A6|   automatic|black|\n",
            "+-----------------+----------+------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#distinct_count\n",
        "df.select(\"manufacturer_name\").distinct().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivbmHGBso7CO",
        "outputId": "9213e202-6f93-44b0-96c9-27ce05529fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP6fU0JvDtOs",
        "outputId": "5febdf13-69ff-4caf-b24b-cbf9a8329b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38531"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GROUPBY() and ORDERBY()\n",
        "df.groupBy(\"manufacturer_name\").count().orderBy(F.col(\"count\").desc()).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA13R6gdDxIU",
        "outputId": "993dcd21-57cd-4de3-bcc2-e8e8ae3440fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|manufacturer_name|count|\n",
            "+-----------------+-----+\n",
            "|       Volkswagen| 4243|\n",
            "|             Opel| 2759|\n",
            "|              BMW| 2610|\n",
            "|             Ford| 2566|\n",
            "|          Renault| 2493|\n",
            "+-----------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Переименовываем колонку\n",
        "df = df.withColumnRenamed(\"manufacturer_name\", \"manufacturer\")"
      ],
      "metadata": {
        "id": "2WSjQvQGFwWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Создаем новую колонку\n",
        "df = df.withColumn(\"next_year\", F.col(\"year_produced\") - 1)"
      ],
      "metadata": {
        "id": "UDFHnn7ucqrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Посмотрим типы колонок\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3O3s4n7dUIG",
        "outputId": "80c2905c-a557-4a98-924a-56b041ae16a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- manufacturer: string (nullable = true)\n",
            " |-- model_name: string (nullable = true)\n",
            " |-- transmission: string (nullable = true)\n",
            " |-- color: string (nullable = true)\n",
            " |-- odometer_value: string (nullable = true)\n",
            " |-- year_produced: string (nullable = true)\n",
            " |-- engine_fuel: string (nullable = true)\n",
            " |-- engine_has_gas: string (nullable = true)\n",
            " |-- engine_type: string (nullable = true)\n",
            " |-- engine_capacity: string (nullable = true)\n",
            " |-- body_type: string (nullable = true)\n",
            " |-- has_warranty: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- drivetrain: string (nullable = true)\n",
            " |-- price_usd: string (nullable = true)\n",
            " |-- is_exchangeable: string (nullable = true)\n",
            " |-- location_region: string (nullable = true)\n",
            " |-- number_of_photos: string (nullable = true)\n",
            " |-- up_counter: string (nullable = true)\n",
            " |-- feature_0: string (nullable = true)\n",
            " |-- feature_1: string (nullable = true)\n",
            " |-- feature_2: string (nullable = true)\n",
            " |-- feature_3: string (nullable = true)\n",
            " |-- feature_4: string (nullable = true)\n",
            " |-- feature_5: string (nullable = true)\n",
            " |-- feature_6: string (nullable = true)\n",
            " |-- feature_7: string (nullable = true)\n",
            " |-- feature_8: string (nullable = true)\n",
            " |-- feature_9: string (nullable = true)\n",
            " |-- duration_listed: string (nullable = true)\n",
            " |-- next_year: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#посмотрим метрики по чиловым столбцам\n",
        "df.select('odometer_value', 'year_produced', 'engine_capacity', 'price_usd', 'number_of_photos', 'up_counter', 'duration_listed').describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txlEsW8XDdBP",
        "outputId": "5a8ab483-0246-460f-dcd2-a7f40fb3ae90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
            "|summary|    odometer_value|     year_produced|  engine_capacity|        price_usd| number_of_photos|        up_counter|   duration_listed|\n",
            "+-------+------------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
            "|  count|             38531|             38531|            38521|            38531|            38531|             38531|             38531|\n",
            "|   mean| 248864.6384469648|2002.9437336170874|2.055161106928777|6639.971021255605|9.649061794399314|16.306091199294073|  80.5772494874257|\n",
            "| stddev|136072.37652978086| 8.065730511309935|0.671177667208744|6428.152018202911|6.093216996872852| 43.28693309422311|112.82656864261321|\n",
            "|    min|                 0|              1942|              0.2|              1.0|                1|                 1|                 0|\n",
            "|    max|            999999|              2019|              8.0|           9999.0|                9|                99|               995|\n",
            "+-------+------------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#изменим тыпы колонок\n",
        "df.withColumn(\"odometer_value\", df[\"odometer_value\"].cast(IntegerType()))\\\n",
        "  .withColumn(\"year_produced\", df[\"year_produced\"].cast(IntegerType()))\\\n",
        "  .withColumn(\"engine_capacity\", df[\"engine_capacity\"].cast(FloatType()))\\\n",
        "  .withColumn(\"price_usd\", df[\"price_usd\"].cast(FloatType()))\\\n",
        "  .withColumn(\"number_of_photos\", df[\"number_of_photos\"].cast(IntegerType()))\\\n",
        "  .withColumn(\"up_counter\", df[\"up_counter\"].cast(IntegerType()))\\\n",
        "  .withColumn(\"duration_listed\", df[\"duration_listed\"].cast(IntegerType()))\\\n",
        "  .withColumn(\"next_year\", df[\"next_year\"].cast(IntegerType()))\\\n",
        "  .printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT1AkroKDst8",
        "outputId": "1736d15a-55ff-4a58-f8dc-0b6774e0c985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- manufacturer: string (nullable = true)\n",
            " |-- model_name: string (nullable = true)\n",
            " |-- transmission: string (nullable = true)\n",
            " |-- color: string (nullable = true)\n",
            " |-- odometer_value: integer (nullable = true)\n",
            " |-- year_produced: integer (nullable = true)\n",
            " |-- engine_fuel: string (nullable = true)\n",
            " |-- engine_has_gas: string (nullable = true)\n",
            " |-- engine_type: string (nullable = true)\n",
            " |-- engine_capacity: float (nullable = true)\n",
            " |-- body_type: string (nullable = true)\n",
            " |-- has_warranty: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- drivetrain: string (nullable = true)\n",
            " |-- price_usd: float (nullable = true)\n",
            " |-- is_exchangeable: string (nullable = true)\n",
            " |-- location_region: string (nullable = true)\n",
            " |-- number_of_photos: integer (nullable = true)\n",
            " |-- up_counter: integer (nullable = true)\n",
            " |-- feature_0: string (nullable = true)\n",
            " |-- feature_1: string (nullable = true)\n",
            " |-- feature_2: string (nullable = true)\n",
            " |-- feature_3: string (nullable = true)\n",
            " |-- feature_4: string (nullable = true)\n",
            " |-- feature_5: string (nullable = true)\n",
            " |-- feature_6: string (nullable = true)\n",
            " |-- feature_7: string (nullable = true)\n",
            " |-- feature_8: string (nullable = true)\n",
            " |-- feature_9: string (nullable = true)\n",
            " |-- duration_listed: integer (nullable = true)\n",
            " |-- next_year: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Перейдем к написанию небольшого пайплана\n",
        "\n",
        "Задача: Необходимо сделать пайплайн обработки файла cars.csv . Посчитать по каждому производителю (поле manufacturer_name):\n",
        "\n",
        "*   Количество объявлений\n",
        "*   Средний год выпуска автомобилей\n",
        "*   Минимальную цену\n",
        "*   Максимальную цену\n",
        "\n",
        "Выгрузить результат в output.csv\n"
      ],
      "metadata": {
        "id": "HZGK4T2Z9qAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Напишем фнкцию, которая производит обработку и выгружает данные\n",
        "def create_output():\n",
        "    output = (\n",
        "        df\n",
        "        .groupBy('manufacturer')\n",
        "        .agg(\n",
        "            F.count('manufacturer').alias('Count'),\n",
        "            F.round(F.avg('year_produced')).cast(IntegerType()).alias('Avarage'),\n",
        "            F.min(F.col('price_usd').cast(FloatType())).alias('Min_price'),\n",
        "            F.max(F.col('price_usd').cast(FloatType())).alias('Max_price')\n",
        "            )\n",
        "        )\n",
        "    output.show(5)\n",
        "    dt = pd.DataFrame(output.take(5), columns=output.columns)\n",
        "    dt.to_csv('filename.csv', index=False)\n",
        "    files.download('filename.csv')\n",
        "    return dt\n",
        "\n",
        "create_output()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "7fxtVqBStqOp",
        "outputId": "0c39349f-b454-4b65-efe3-0c989a502888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+-------+---------+---------+\n",
            "|manufacturer|Count|Avarage|Min_price|Max_price|\n",
            "+------------+-----+-------+---------+---------+\n",
            "|  Volkswagen| 4243|   2002|      1.0|  43999.0|\n",
            "|       Lexus|  213|   2008|   2500.0| 48610.45|\n",
            "|      Jaguar|   53|   2009|   2500.0|  50000.0|\n",
            "|       Rover|  235|   1998|    200.0|   9900.0|\n",
            "|      Lancia|   92|   2000|    200.0|   9500.0|\n",
            "+------------+-----+-------+---------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_49b9a373-0daf-4cc6-aa47-7a986c8efac9\", \"filename.csv\", 203)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  manufacturer  Count  Avarage  Min_price     Max_price\n",
              "0   Volkswagen   4243     2002        1.0  43999.000000\n",
              "1        Lexus    213     2008     2500.0  48610.449219\n",
              "2       Jaguar     53     2009     2500.0  50000.000000\n",
              "3        Rover    235     1998      200.0   9900.000000\n",
              "4       Lancia     92     2000      200.0   9500.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6706e33d-076c-4f15-a590-abc05ed1d34d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>Count</th>\n",
              "      <th>Avarage</th>\n",
              "      <th>Min_price</th>\n",
              "      <th>Max_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Volkswagen</td>\n",
              "      <td>4243</td>\n",
              "      <td>2002</td>\n",
              "      <td>1.0</td>\n",
              "      <td>43999.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lexus</td>\n",
              "      <td>213</td>\n",
              "      <td>2008</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>48610.449219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jaguar</td>\n",
              "      <td>53</td>\n",
              "      <td>2009</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>50000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Rover</td>\n",
              "      <td>235</td>\n",
              "      <td>1998</td>\n",
              "      <td>200.0</td>\n",
              "      <td>9900.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lancia</td>\n",
              "      <td>92</td>\n",
              "      <td>2000</td>\n",
              "      <td>200.0</td>\n",
              "      <td>9500.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6706e33d-076c-4f15-a590-abc05ed1d34d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6706e33d-076c-4f15-a590-abc05ed1d34d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6706e33d-076c-4f15-a590-abc05ed1d34d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-751982e1-7a14-4324-ad54-cdc553f2c3fa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-751982e1-7a14-4324-ad54-cdc553f2c3fa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-751982e1-7a14-4324-ad54-cdc553f2c3fa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"create_output()\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"manufacturer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Lexus\",\n          \"Lancia\",\n          \"Jaguar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1832,\n        \"min\": 53,\n        \"max\": 4243,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          213,\n          92,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Avarage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 1998,\n        \"max\": 2009,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2008,\n          2000,\n          2009\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Min_price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1298.637824799509,\n        \"min\": 1.0,\n        \"max\": 2500.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          2500.0,\n          200.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Max_price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20843.077070193627,\n        \"min\": 9500.0,\n        \"max\": 50000.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          48610.44921875,\n          9500.0,\n          50000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Остонавливаем сессию спарк\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "rDoHT5sU8UfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Поиск M&Ms\n",
        "\n",
        "Дан сsv файл, который подтягивается из гугл диска.\n",
        "Задача посчитать количество M&Ms по цветовой гамме в пачках в разных штатах."
      ],
      "metadata": {
        "id": "vr-e9pQcLXpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Библиотеки\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count\n",
        "import requests\n",
        "\n",
        "#ссылка на загрузку CSV файла c гугл Диска\n",
        "url = f'https://drive.google.com/uc?id=1oZTMFL05rLTN-oqBXl4W-rWS-9bpcphg'\n",
        "\n",
        "#Загружаем файл\n",
        "response = requests.get(url)\n",
        "#Проверяем статус ответа, если ок, то открываем файл в памяти\n",
        "if response.status_code == 200:\n",
        "    with open('mnm.csv', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Файл загружен\")\n",
        "else:\n",
        "    print(f\"Файл не загрузился {response.status_code}\")"
      ],
      "metadata": {
        "id": "PuKwqY1D0Rm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#выводим название файла\n",
        "file_name = str(f).split(\"name='\")[1].split(\"'\")[0]\n",
        "#вызываем функцию, которая генерит объект объект Spark\n",
        "df = create_spark_object(file_name)\n",
        "\n",
        "#посмотрим кол-во конфет в разрезе штатов и цветовой гаммы\n",
        "count_mnm_state_color = (df.select('State', 'Color', 'Count')\n",
        "                           .groupBy('State', 'Color')\n",
        "                           .agg(count(\"Count\").alias(\"Total\"))\n",
        "                           .orderBy(\"Total\", ascending=False).show())\n",
        "\n",
        "#проверим конфеты по Калифорнии\n",
        "count_mnm_NY = (df.select('State','Color','Count')\n",
        "                  .where(df.State == 'CA')\n",
        "                  .groupBy('State', 'Color')\n",
        "                  .agg(count(\"Count\").alias('Total'))\n",
        "                  .orderBy(\"Total\", ascending=False)\n",
        "                  .show())\n",
        "count_mnm_NY"
      ],
      "metadata": {
        "id": "WTvIpeq40Q6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Поиск слова Spark в тексте.\n",
        "\n",
        "Дан .md файл с текстом. Задача, выгрузить строки, где есть слово Spark из файла md формата."
      ],
      "metadata": {
        "id": "GW4jcniqLmjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Выгрузка библиотек\n",
        "from pyspark.sql import SparkSession\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "#обращаемся к гугл диску\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#создаем спарк приложение и читаем файл\n",
        "spark = SparkSession.builder.config('spark.ui.port', '4050').appName('Test').getOrCreate()\n",
        "file_path = '/content/drive/My Drive/README.md'\n",
        "strings = spark.read.text(file_path)\n",
        "\n",
        "#выводим строки с неоьходимым нам словом\n",
        "filtered = strings.filter(strings.value.contains(\"Spark\"))\n",
        "filtered.show(100)"
      ],
      "metadata": {
        "id": "XYl1I6P44mmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Останавливаем сессию\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "t4lZtYDvMTRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Создаем схему\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mzvWUt6_5r4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Задание\n",
        "Необходимо сгенерить данные и создать под них схему."
      ],
      "metadata": {
        "id": "VoONM_lVaIs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Выгружаем библиотеку\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "# Прописываем схему DDL\n",
        "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
        "# Данные для датафрейма\n",
        "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
        " [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
        "\"LinkedIn\"]],\n",
        " [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
        "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
        " [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
        "[\"twitter\", \"FB\"]],\n",
        " [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
        "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
        " [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
        "[\"twitter\", \"LinkedIn\"]]\n",
        " ]\n",
        "spark = (SparkSession\n",
        " .builder\n",
        " .appName(\"Example-3_6\")\n",
        " .getOrCreate())\n",
        "\n",
        "# Создаем датафрейм используя данные и схему\n",
        "blogs_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "#Выводим данные и структуру схемы\n",
        "blogs_df.show()\n",
        "print(blogs_df.schema)\n",
        "print(' ')\n",
        "blogs_df.printSchema()"
      ],
      "metadata": {
        "id": "8ZF4tcaC5wbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blogs_df\\\n",
        "       .withColumn('Big_Hints', F.col('Hits') > 5000)\\\n",
        "       .withColumn('Auth_data', (F.concat(F.col('First'), F.col('Last'), F.col('Id'))))\\\n",
        "       .sort(F.col('Hits').desc())\\\n",
        "       .show()"
      ],
      "metadata": {
        "id": "yvE-OwKC55S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Задание\n",
        "Необходимо создать схему и положить в нее данные из GitHub. <a name=\"4.2\">"
      ],
      "metadata": {
        "id": "5eXnbdBJaTv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#Моделим схему\n",
        "file_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
        "                          StructField('UnitID', StringType(), True),\n",
        "                          StructField('InciedentNumber', IntegerType(),True),\n",
        "                          StructField('CallType', StringType(), True),\n",
        "                          StructField('CallDate', StringType(), True),\n",
        "                          StructField('WatchDate', StringType(), True),\n",
        "                          StructField('CallFinalDisposition', StringType(), True),\n",
        "                          StructField('AvailableDtTm', StringType(), True),\n",
        "                          StructField('Address', StringType(), True),\n",
        "                          StructField('City', StringType(), True),\n",
        "                          StructField('Zipcode', IntegerType(), True),\n",
        "                          StructField('Battalion', StringType(), True),\n",
        " StructField('StationArea', StringType(), True),\n",
        " StructField('Box', StringType(), True),\n",
        " StructField('OriginalPriority', StringType(), True),\n",
        " StructField('Priority', StringType(), True),\n",
        " StructField('FinalPriority', IntegerType(), True),\n",
        " StructField('ALSUnit', BooleanType(), True),\n",
        " StructField('CallTypeGroup', StringType(), True),\n",
        " StructField('NumAlarms', IntegerType(), True),\n",
        " StructField('UnitType', StringType(), True),\n",
        " StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
        " StructField('FirePreventionDistrict', StringType(), True),\n",
        " StructField('SupervisorDistrict', StringType(), True),\n",
        " StructField('Neighborhood', StringType(), True),\n",
        " StructField('Location', StringType(), True),\n",
        " StructField('RowID', StringType(), True),\n",
        " StructField('Delay', FloatType(), True)])\n",
        "\n",
        "#Читаем датафрейм CSV из GitHub\n",
        "spark = SparkSession.builder.config('spark.ui.port', '4050').appName('Test').getOrCreate()\n",
        "str_file = \"/content/LearningSparkV2/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\"\n",
        "fire_df = spark.read.csv(str_file, header = True, schema = file_schema)\n",
        "\n",
        "fire_df.show(5)\n",
        "fire_df.printSchema()"
      ],
      "metadata": {
        "id": "GCVulZC7apTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Работа с строками\n",
        "\n",
        "Создать датафрейм из объекта Row"
      ],
      "metadata": {
        "id": "C7_Ub6csGWLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "#содержимое строки\n",
        "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
        "author_row = spark.createDataFrame(rows, [\"Authors\",\"Region\"])\n",
        "author_row.show()"
      ],
      "metadata": {
        "id": "SNpQV0p_ERPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Поиск не пустых значений\n",
        "\n",
        "Нужно посмотреть причины вызовов пожарных частей - значения должны быть не Null.\n",
        "\n",
        "Для выпонения надо выполнить [4.2](#4.2)  \n",
        "\n"
      ],
      "metadata": {
        "id": "8Ov24g2eMIa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Читаем датафрейм CSV из GitHub\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "fire_df.select(F.col(\"CallType\"))\\\n",
        "       .filter(F.col(\"CallType\").isNotNull())\\\n",
        "       .groupBy(F.col(\"CallType\"))\\\n",
        "       .agg(F.count('CallType').alias('Count'))\\\n",
        "       .orderBy(F.col(\"Count\").desc())\\\n",
        "       .show(100,False)"
      ],
      "metadata": {
        "id": "uPxovH6dPzos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Замена столбцов и их удаление\n",
        "\n",
        "Дан датафрейм из Задачи [4.2](#4.2) необходимо изменить тип временных колонок для дальнейшей работы  с ними. Таже необходимо подготовить ответы на вопросы -\n",
        "* Посмотреть за какие года у были инциденты.\n",
        "* Какие виды пожара были в 2018 году.\n",
        "* В какие месяцы 2018 года было больше всего вызовов пожарных?\n",
        "* В каком районе Сан Франциско произошло больше всего звонков о пожаре в 2018?\n",
        "* В каких районах было худшее время реагирования пожарных в 2018 году?\n",
        "* На какой неделе 2018 года было больше всего вызовов пожарных?\n",
        "* Как использовать тип файлов Паркет, для хранения этих данных."
      ],
      "metadata": {
        "id": "eUdrQfVXfQdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача У всех временных столбцов тип поля string\n",
        "fire_df.select('CallDate','WatchDate','AvailableDtTm').printSchema()\n",
        "\n",
        "#заменим типы и удалим старые колонки\n",
        "fire_df = (fire_df\n",
        "           .withColumn(\"IncidentDate\", F.to_timestamp(\"CallDate\", \"MM/dd/yyyy\"))\n",
        "           .drop(\"CallDate\")\n",
        "           .withColumn(\"OnWatchDate\", F.to_timestamp(\"WatchDate\",\"MM/dd/yyyy\"))\n",
        "           .drop(\"WatchDate\")\n",
        "           .withColumn(\"AvailableDtTS\", F.to_timestamp(\"AvailableDtTm\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
        "           .drop(\"AvailableDtTm\"))\n",
        "\n",
        "#Проверка\n",
        "fire_df.printSchema()"
      ],
      "metadata": {
        "id": "ZZPgl6WogWFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 1 за какие года были инциденты\n",
        "fire_df.select(F.year('IncidentDate').alias('Year')).distinct().orderBy(F.year('IncidentDate')).show(100,False)"
      ],
      "metadata": {
        "id": "EWgxaSMfMHWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 2 какие виды пожаров были в 2018\n",
        "fire_df.select(F.col(\"CallType\"))\\\n",
        "       .filter(F.year('IncidentDate') == 2018)\\\n",
        "       .groupBy(F.col(\"CallType\"))\\\n",
        "       .agg(F.count('CallType').alias('Count'))\\\n",
        "       .orderBy(F.col(\"Count\").desc())\\\n",
        "       .show(100,False)"
      ],
      "metadata": {
        "id": "EpLc6FUrOdmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 3 в какие месяцы 2018 года было больше всего вызовов пожарных\n",
        "a = (fire_df.select(F.month(\"IncidentDate\").alias('MONTH'), F.year('IncidentDate').alias('year'))\n",
        "            .filter(F.year('IncidentDate') == 2018)\n",
        "            .groupBy('MONTH', 'year')\n",
        "            .agg(F.count('*').alias('count_calls'))\n",
        "            .orderBy(F.col('count_calls').desc())\n",
        "            .show(100,False))\n",
        "a"
      ],
      "metadata": {
        "id": "RWSzGYrCR67U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 4 в каком районе Сан Франциско произошло больше всего звонков о пожаре в 2018?\n",
        "a = (fire_df\n",
        "           .select('Neighborhood', 'NumAlarms')\n",
        "           .where((F.col('City') == 'San Francisco') & (F.year('IncidentDate') == 2018))\n",
        "           .groupBy('Neighborhood')\n",
        "           .agg(F.sum('NumAlarms').alias('Count_calls'))\n",
        "           .orderBy(F.col('Count_calls').desc())\n",
        "           .show(1, False))\n",
        "a"
      ],
      "metadata": {
        "id": "wGV6d4B-Yo7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Задание 5 в каких районах было худшее время реагирования пожарных в 2018 году?\n",
        "a = (fire_df\n",
        "           .select('City', 'Neighborhood', 'Delay')\n",
        "           .filter(F.year('IncidentDate') == 2018)\n",
        "           .orderBy(F.col('Delay').desc())\n",
        "           .show(5, False))"
      ],
      "metadata": {
        "id": "g8GNjpoBNA4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Задание 6 на какой неделе 2018 года было больше всего вызовов пожарных?\n",
        "a = (fire_df\n",
        "           .select('IncidentDate', 'NumAlarms')\n",
        "           .filter(F.year('IncidentDate') == 2018)\n",
        "           .groupBy(F.weekofyear('IncidentDate'))\n",
        "           .agg(F.sum('NumAlarms').alias('sums'))\n",
        "           .orderBy(F.col('sums').desc())\n",
        "           .show(5, False))"
      ],
      "metadata": {
        "id": "jUzLk1GAQsaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Задание 7 как использовать тип файлов Паркет, для хранения этих данных.\n",
        "fire_df.write.format('parquet').save('/content/sample_data/qwe')"
      ],
      "metadata": {
        "id": "AevZf_bWVyp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "row = Row(350, True, \"Learning Spark 2E\", None)\n",
        "row[2]"
      ],
      "metadata": {
        "id": "v015VjLtdSov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. SparkSQL совместимость с DataFrame\n",
        "\n",
        "Необходимо прочесть данные во временном представлении. С помощью sql."
      ],
      "metadata": {
        "id": "zMynzBB7fBc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#импорт библиотеки\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#открываем сессию\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Hive options in PySpark\") \\\n",
        "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.0.1\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#ссылка на датассет\n",
        "csv_file = '/content/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/departuredelays.csv'\n",
        "\n",
        "#делаем датафрейм в Спарке\n",
        "df = (spark.read.format(\"csv\")\n",
        "                .option(\"inferSchema\", \"true\")\n",
        "                .option(\"header\", \"true\")\n",
        "                .load(csv_file))\n",
        "\n",
        "#создаем временную таблицу\n",
        "df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n",
        "\n",
        "#создаем схему\n",
        "schema = \"'date' STRING, 'delay' INT, 'distance' INT, 'origin' STRING, 'destination' STRING\"\n",
        "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-CZSSlQfhIx",
        "outputId": "b9c6a1d4-12e1-466f-fad1-f50beb3a5f53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+------+-----------+\n",
            "|   date|delay|distance|origin|destination|\n",
            "+-------+-----+--------+------+-----------+\n",
            "|1011245|    6|     602|   ABE|        ATL|\n",
            "|1020600|   -8|     369|   ABE|        DTW|\n",
            "|1021245|   -2|     602|   ABE|        ATL|\n",
            "|1020605|   -4|     602|   ABE|        ATL|\n",
            "|1031245|   -4|     602|   ABE|        ATL|\n",
            "+-------+-----+--------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* date - cтолбец даты содержит строку типа 02190925. При преобразовании это соответствует 02–19 09:25.\n",
        "\n",
        "* delay - в столбце задержки указана задержка в минутах между запланированным и фактическим временем.\n",
        "время отправления. Ранние вылеты показывают отрицательные числа.\n",
        "\n",
        "* distance - в столбце «Расстояние» указано расстояние в милях от аэропорта отправления до пункта отправления.\n",
        "аэропорт назначения.\n",
        "\n",
        "* origin - cтолбец происхождения содержит код аэропорта отправления IATA.\n",
        "\n",
        "* destination - cтолбец назначения содержит код аэропорта назначения IATA."
      ],
      "metadata": {
        "id": "cV1WbaXvn99B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 1. Найдем все рейсы где расстояние превышает 1000 миль.\n",
        "spark.sql(\"SELECT distance, origin, destination \\\n",
        "           FROM us_delay_flights_tbl \\\n",
        "           WHERE distance > 1000 \\\n",
        "           ORDER BY distance DESC\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7wrhHcAghx8",
        "outputId": "ddfd2f8e-e057-4c1c-fcfe-9c11ff1aae51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----------+\n",
            "|distance|origin|destination|\n",
            "+--------+------+-----------+\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "+--------+------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все самые длинные перелеты были между Гонолулу (HNL) и Нью-Йорком.\n",
        "Йорк (JFK)."
      ],
      "metadata": {
        "id": "2X0kf89nr4Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 2. Найдем все рейсы между Сан-Франциско (SFO) и Чикаго. (ORD) с задержкой не менее двух часов\n",
        "spark.sql(\"SELECT * FROM us_delay_flights_tbl \\\n",
        "           WHERE origin = 'SFO' AND destination = 'ORD' \\\n",
        "                 AND delay > 120 \\\n",
        "           ORDER BY delay DESC\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_Uk6A5brghc",
        "outputId": "f9f49b53-6345-4b5c-a92c-30e2c2d86051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+------+-----------+\n",
            "|   date|delay|distance|origin|destination|\n",
            "+-------+-----+--------+------+-----------+\n",
            "|2190925| 1638|    1604|   SFO|        ORD|\n",
            "|1031755|  396|    1604|   SFO|        ORD|\n",
            "|1022330|  326|    1604|   SFO|        ORD|\n",
            "|1051205|  320|    1604|   SFO|        ORD|\n",
            "|1190925|  297|    1604|   SFO|        ORD|\n",
            "+-------+-----+--------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 3. Пометим все рейсы в США, независимо от пункта отправления и назначения, с указанием возникших задержек:\n",
        "#очень длительные задержки (> 6 часов), Длительные задержки (2–6 часов) и т. д.\n",
        "spark.sql(\"SELECT delay, origin, destination, \\\n",
        "             CASE \\\n",
        "                 WHEN delay > 360 THEN 'Very Long Delays' \\\n",
        "                 WHEN delay > 120 AND delay < 360 THEN 'Long Delays' \\\n",
        "                 WHEN delay > 60 AND delay < 120 THEN 'Short Delays' \\\n",
        "                 WHEN delay > 0 AND delay < 60 THEN 'Tolerable Delays' \\\n",
        "                 WHEN delay = 0 THEN 'No Delays' \\\n",
        "                 ELSE 'Early' \\\n",
        "              END AS Flight_Delays \\\n",
        "           FROM us_delay_flights_tbl \\\n",
        "           ORDER BY origin, delay DESC\").show(18)\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XX5W9DbsSDi",
        "outputId": "0a54879f-8201-4fd7-cca5-bacff07a2acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+-------------+\n",
            "|delay|origin|destination|Flight_Delays|\n",
            "+-----+------+-----------+-------------+\n",
            "|  333|   ABE|        ATL|  Long Delays|\n",
            "|  305|   ABE|        ATL|  Long Delays|\n",
            "|  275|   ABE|        ATL|  Long Delays|\n",
            "|  257|   ABE|        ATL|  Long Delays|\n",
            "|  247|   ABE|        ATL|  Long Delays|\n",
            "|  247|   ABE|        DTW|  Long Delays|\n",
            "|  219|   ABE|        ORD|  Long Delays|\n",
            "|  211|   ABE|        ATL|  Long Delays|\n",
            "|  197|   ABE|        DTW|  Long Delays|\n",
            "|  192|   ABE|        ORD|  Long Delays|\n",
            "|  180|   ABE|        ATL|  Long Delays|\n",
            "|  173|   ABE|        DTW|  Long Delays|\n",
            "|  165|   ABE|        ATL|  Long Delays|\n",
            "|  159|   ABE|        ORD|  Long Delays|\n",
            "|  159|   ABE|        ATL|  Long Delays|\n",
            "|  158|   ABE|        ATL|  Long Delays|\n",
            "|  151|   ABE|        DTW|  Long Delays|\n",
            "|  127|   ABE|        ATL|  Long Delays|\n",
            "+-----+------+-----------+-------------+\n",
            "only showing top 18 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 4. Создать бд и поселить в ней таблицу\n",
        "\n",
        "#импорт библиотеки\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Create spark session with hive enabled (изменен порядок)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Hive options in PySpark\") \\\n",
        "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.0.1\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#создаем бд\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS learn_spark_db\")\n",
        "\n",
        "#задействуем ее\n",
        "spark.sql(\"USE learn_spark_db\")\n",
        "\n",
        "#создаем управляемую таблицу\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")\n",
        "spark.sql(\"SELECT * FROM managed_us_delay_flights_tbl\").show(5)\n",
        "\n",
        "#также можно создать неуправляемую таблицу\n",
        "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS us_delay_flights_tbl(date STRING, delay INT,\n",
        " distance INT, origin STRING, destination STRING)\n",
        " USING csv OPTIONS (PATH\n",
        " '/content/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")\n",
        "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)"
      ],
      "metadata": {
        "id": "2ti2JtZLwdjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfdd7696-dd2a-4ed6-f101-04e31ec57141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+--------+------+-----------+\n",
            "|date|delay|distance|origin|destination|\n",
            "+----+-----+--------+------+-----------+\n",
            "+----+-----+--------+------+-----------+\n",
            "\n",
            "+--------+-----+--------+------+-----------+\n",
            "|    date|delay|distance|origin|destination|\n",
            "+--------+-----+--------+------+-----------+\n",
            "|    date| NULL|    NULL|origin|destination|\n",
            "|01011245|    6|     602|   ABE|        ATL|\n",
            "|01020600|   -8|     369|   ABE|        DTW|\n",
            "|01021245|   -2|     602|   ABE|        ATL|\n",
            "|01020605|   -4|     602|   ABE|        ATL|\n",
            "+--------+-----+--------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Глобальне и временные представления"
      ],
      "metadata": {
        "id": "gPrKXnkrUgeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 5. Создать глобальную и временную вьюху\n",
        "df_sfo_GLOB = spark.sql(\"SELECT date, delay, origin, destination \\\n",
        "                         FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
        "df_sfo_GLOB.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
        "print(\"Глобальное представление\")\n",
        "spark.sql('SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view').show(5)\n",
        "\n",
        "\n",
        "df_jfk = spark.sql(\"SELECT date, delay, origin, destination \\\n",
        "                    FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
        "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\n",
        "print(\"Локальное представление\")\n",
        "spark.sql('SELECT * FROM us_origin_airport_JFK_tmp_view').show(5)\n",
        "\n",
        "#метаинфа по базе данных\n",
        "spark.catalog.listDatabases()\n",
        "spark.catalog.listTables()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8YQuxPMZLXE",
        "outputId": "69d6b1d6-d189-4972-896a-a2834767077b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Глобальное представление\n",
            "+--------+-----+------+-----------+\n",
            "|    date|delay|origin|destination|\n",
            "+--------+-----+------+-----------+\n",
            "|01011250|   55|   SFO|        JFK|\n",
            "|01012230|    0|   SFO|        JFK|\n",
            "|01010705|   -7|   SFO|        JFK|\n",
            "|01010620|   -3|   SFO|        MIA|\n",
            "|01010915|   -3|   SFO|        LAX|\n",
            "+--------+-----+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Локальное представление\n",
            "+--------+-----+------+-----------+\n",
            "|    date|delay|origin|destination|\n",
            "+--------+-----+------+-----------+\n",
            "|01010900|   14|   JFK|        LAX|\n",
            "|01011200|   -3|   JFK|        LAX|\n",
            "|01011900|    2|   JFK|        LAX|\n",
            "|01011700|   11|   JFK|        LAS|\n",
            "|01010800|   -1|   JFK|        SFO|\n",
            "+--------+-----+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='managed_us_delay_flights_tbl', catalog='spark_catalog', namespace=['learn_spark_db'], description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='us_delay_flights_tbl', catalog='spark_catalog', namespace=['learn_spark_db'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
              " Table(name='us_origin_airport_JFK_tmp_view', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 5 (1) тоже самое на SQL\n",
        "spark.sql(\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
        "spark.sql(\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\").show(5)\n",
        "\n",
        "spark.sql(\"CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
        "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\").show(5)\n",
        "\n",
        "spark.sql('DROP TABLE us_origin_airport_JFK_tmp_view')\n",
        "spark.sql('DROP TABLE global_temp.us_origin_airport_SFO_global_tmp_view')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhLEeUalPpYq",
        "outputId": "5a1117fb-5183-4fef-d207-d050713408a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+------+-----------+\n",
            "|    date|delay|origin|destination|\n",
            "+--------+-----+------+-----------+\n",
            "|01011250|   55|   SFO|        JFK|\n",
            "|01012230|    0|   SFO|        JFK|\n",
            "|01010705|   -7|   SFO|        JFK|\n",
            "|01010620|   -3|   SFO|        MIA|\n",
            "|01010915|   -3|   SFO|        LAX|\n",
            "+--------+-----+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------+-----+------+-----------+\n",
            "|    date|delay|origin|destination|\n",
            "+--------+-----+------+-----------+\n",
            "|01010900|   14|   JFK|        LAX|\n",
            "|01011200|   -3|   JFK|        LAX|\n",
            "|01011900|    2|   JFK|        LAX|\n",
            "|01011700|   11|   JFK|        LAS|\n",
            "|01010800|   -1|   JFK|        SFO|\n",
            "+--------+-----+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метаинформация по таблицам, листам, бд"
      ],
      "metadata": {
        "id": "jIAUA96YUvij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Метаинфа по БД\n",
        "spark.catalog.listDatabases()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_xMshcoXei3",
        "outputId": "e613be2d-87e9-40bc-e45e-80db8ba6133b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/content/spark-warehouse'),\n",
              " Database(name='learn_spark_db', catalog='spark_catalog', description='', locationUri='file:/content/spark-warehouse/learn_spark_db.db')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Метаинфа по таблицам\n",
        "spark.catalog.listTables()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCu6_Rw1WclI",
        "outputId": "89e05b0d-5372-4545-c2ca-ff19a033da84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='managed_us_delay_flights_tbl', catalog='spark_catalog', namespace=['learn_spark_db'], description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='us_delay_flights_tbl', catalog='spark_catalog', namespace=['learn_spark_db'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
              " Table(name='us_origin_airport_JFK_tmp_view', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#метаинфа по определенной таблице в бд\n",
        "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JClOAgXuXr_2",
        "outputId": "78a80e7c-3b91-4103-9801-d4cd5e560e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Чтение и запись данных из файлов Parquet, CSV, JSON,"
      ],
      "metadata": {
        "id": "MpjX_xmQVCZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 6 Создать неуправляемую таблицу и посмотреть Parquet файл напрямую с помощью SQL\n",
        "\n",
        "#чтение файла\n",
        "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl \\\n",
        "           USING parquet \\\n",
        "           OPTIONS (path '/content/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet')\")\n",
        "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)\n",
        "spark.sql(\"DROP TABLE us_delay_flights_tbl\")\n",
        "\n",
        "#запись файла\n",
        "df_jfk.write.format(\"parquet\").mode(\"overwrite\").option(\"compression\", \"snappy\").save(\"/content/LearningSparkV2/tmp/parquet/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFbzfnooQ-ZU",
        "outputId": "29b1a2de-1597-459e-b7fa-bf8668f991c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 7 Создать неуправляемую таблицу и посмотреть JSON файл напрямую с помощью SQL\n",
        "\n",
        "#чтение файла\n",
        "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl \\\n",
        "           USING json \\\n",
        "           OPTIONS (path '/content/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/summary-data/json/*')\")\n",
        "\n",
        "#вывод файла\n",
        "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)\n",
        "spark.sql(\"DROP TABLE us_delay_flights_tbl\")\n",
        "\n",
        "#запись файла\n",
        "df_jfk.write.format(\"JSON\").mode(\"overwrite\").option(\"compression\", \"snappy\").save(\"/content/LearningSparkV2/tmp/JSON/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_usXEC7O3LU0",
        "outputId": "b93a9163-b73a-4655-df15-420301a78def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 8 Создать неуправляемую таблицу и посмотреть CSV файл напрямую с помощью SQL\n",
        "\n",
        "#чтение файла\n",
        "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl \\\n",
        "           USING csv \\\n",
        "           OPTIONS \\\n",
        "           (path '/content/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/summary-data/csv', \\\n",
        "            header 'true', \\\n",
        "            inferSchema 'true', \\\n",
        "            mode 'FAILFAST')\")\n",
        "\n",
        "#вывод файла\n",
        "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)\n",
        "spark.sql(\"DROP TABLE us_delay_flights_tbl\")\n",
        "\n",
        "#запись файла\n",
        "df_jfk.write.format(\"CSV\").mode(\"overwrite\").option(\"header\", \"true\").save(\"/content/LearningSparkV2/tmp/CSV/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F8Z0rKEddql",
        "outputId": "f744a68f-c6c1-4391-869b-881e95291552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 9 Создать неуправляемую таблицу и посмотреть ORC файл напрямую с помощью SQL\n",
        "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl \\\n",
        "             USING orc \\\n",
        "             OPTIONS (path '/content/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/summary-data/orc/2010-summary.orc')\")\n",
        "\n",
        "#вывод файла\n",
        "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)\n",
        "spark.sql(\"DROP TABLE us_delay_flights_tbl\")\n",
        "\n",
        "#запись файла\n",
        "df_jfk.write.format(\"orc\").mode(\"overwrite\").save(\"/content/LearningSparkV2/tmp/orc/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6Q86iOEBvn1",
        "outputId": "946a1391-5857-4a15-fa67-579acdd3f575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Использование функций"
      ],
      "metadata": {
        "id": "PW9dPUruJ24z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Задача 10 Создать функцию, которая будет датафрейм и выполнять обработку\n",
        "\n",
        "#импортируем типы их нужно указать в регистраторе функции\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "def cud(s):\n",
        "  return s * s * s\n",
        "\n",
        "#регистратор функции\n",
        "spark.udf.register(\"cud\", cud, LongType())\n",
        "\n",
        "#создаем датафрейм с данными\n",
        "spark.range(1,9).createOrReplaceTempView(\"udf_test\")\n",
        "\n",
        "#используем функцию\n",
        "spark.sql(\"SELECT id, cud(id) as hz FROM udf_test\").show()\n",
        "\n",
        "spark.sql(\"SELECT array_join(array('hello','world'), ' пися ')\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCkehwsx3Pod",
        "outputId": "717d852c-87df-4ed3-9e59-42cc9be22a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "| id| hz|\n",
            "+---+---+\n",
            "|  1|  1|\n",
            "|  2|  8|\n",
            "|  3| 27|\n",
            "|  4| 64|\n",
            "|  5|125|\n",
            "|  6|216|\n",
            "|  7|343|\n",
            "|  8|512|\n",
            "+---+---+\n",
            "\n",
            "+---------------------------------------+\n",
            "|array_join(array(hello, world),  пися )|\n",
            "+---------------------------------------+\n",
            "|                       hello пися world|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import col, pandas_udf\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "def cubed(a: pd.Series) ->pd.Series:\n",
        "  return a * a * a\n",
        "\n",
        "cubed_udf = pandas_udf(cubed, returnType=LongType())\n",
        "x = pd.Series([1, 2, 3])\n",
        "print(cubed(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smzkFJe7Iw-l",
        "outputId": "6d6b9a84-6a87-42e7-cede-fa684419395d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     1\n",
            "1     8\n",
            "2    27\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Функции высшего порядка"
      ],
      "metadata": {
        "id": "65DHizQVOxuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Пример для использования функций\n",
        "from pyspark.sql.types import *\n",
        "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
        "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
        "t_c = spark.createDataFrame(t_list, schema)\n",
        "t_c.createOrReplaceTempView(\"tC\")\n",
        "t_c.show(2, False)"
      ],
      "metadata": {
        "id": "PZTif86XcW_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930dd91e-0ea2-47bb-a536-1b10aba89e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+\n",
            "|celsius                     |\n",
            "+----------------------------+\n",
            "|[35, 36, 32, 30, 40, 42, 38]|\n",
            "|[31, 32, 34, 55, 56]        |\n",
            "+----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transform() создает массив и передает функцию на каждый элемент массива\n",
        "spark.sql(\"\"\"SELECT celsius, transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\\\n",
        "             FROM tC\"\"\").show(2 , False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wepvVuov3Cs7",
        "outputId": "9860ca33-9002-46b0-db07-63ae7cc30848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+-------------------------------+\n",
            "|celsius                     |fahrenheit                     |\n",
            "+----------------------------+-------------------------------+\n",
            "|[35, 36, 32, 30, 40, 42, 38]|[95, 96, 89, 86, 104, 107, 100]|\n",
            "|[31, 32, 34, 55, 56]        |[87, 89, 93, 131, 132]         |\n",
            "+----------------------------+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter() создает массив только из элементов соответствующим булевому значению\n",
        "spark.sql(\"\"\"\n",
        "SELECT celsius,\n",
        " filter(celsius, t -> t > 36) as high\n",
        " FROM tC\n",
        "\"\"\").show(5, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkPQp7x_RUsC",
        "outputId": "1453c8a3-9fc7-43e7-f277-bbf91c4cfe82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+------------+\n",
            "|celsius                     |high        |\n",
            "+----------------------------+------------+\n",
            "|[35, 36, 32, 30, 40, 42, 38]|[40, 42, 38]|\n",
            "|[31, 32, 34, 55, 56]        |[55, 56]    |\n",
            "+----------------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exists() возвращает true если логическая функция выполняется для любого элемента входного массива\n",
        "spark.sql(\"\"\"SELECT celsius, exists(celsius, t -> t = 38) as threshold\\\n",
        "           FROM tC\"\"\").show(3, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGHusHymSooU",
        "outputId": "c09bd681-bcbd-4795-9286-9828f26475c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+---------+\n",
            "|celsius                     |threshold|\n",
            "+----------------------------+---------+\n",
            "|[35, 36, 32, 30, 40, 42, 38]|true     |\n",
            "|[31, 32, 34, 55, 56]        |false    |\n",
            "+----------------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача\n",
        "* Импортируйте два файла и создайте два DataFrame, один для информации об аэропорте (airportsna).\n",
        "и один для задержек рейсов в США (departureDelays)\n",
        "\n",
        "* Используя expr(), преобразуйте столбцы задержки и расстояния из STRING в INT.\n",
        "\n",
        "* Создать небольшую таблицу содержащую только информацию о трёх рейсах, вылетающих из Сиэтла (SEA) в пункт назначения.\n",
        "изменение Сан-Франциско (SFO) на небольшой временной диапазон.\n"
      ],
      "metadata": {
        "id": "pQ1kDyy4fg-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#импорт библиотек\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Create spark session with hive enabled (изменен порядок)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Hive options in PySpark\") \\\n",
        "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.0.1\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "#ссылки на датафреймы\n",
        "airportsnaFILE = \"/content/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
        "\n",
        "departureFILE = \"/content/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
        "\n",
        "#считываем датафреймы\n",
        "#информации об аэропорте\n",
        "airportsna = (spark.read\n",
        "                    .format(\"csv\")\n",
        "                    .options(header = 'True', inferSchema='True', sep = \"\\t\")\n",
        "                    .load(airportsnaFILE))\n",
        "airportsna.show(5)\n",
        "airportsna.createOrReplaceTempView(\"airports_na\")\n",
        "\n",
        "#информации о задержке рейсов\n",
        "departure = (spark.read\n",
        "                  .format(\"csv\")\n",
        "                  .options(header='true', inferSchema='True', sep = \",\")\n",
        "                  .load(departureFILE))\n",
        "departure.show(5)\n",
        "\n",
        "#поменяем формат столбцов\n",
        "departure = (departure\n",
        "                       .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
        "                       .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
        "\n",
        "departure.createOrReplaceTempView(\"departureDelays\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lpsi3v_gIbu",
        "outputId": "a0512a41-3629-4563-f923-73c5ea4a1ef1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-------+----+\n",
            "|      City|State|Country|IATA|\n",
            "+----------+-----+-------+----+\n",
            "|Abbotsford|   BC| Canada| YXX|\n",
            "|  Aberdeen|   SD|    USA| ABR|\n",
            "|   Abilene|   TX|    USA| ABI|\n",
            "|     Akron|   OH|    USA| CAK|\n",
            "|   Alamosa|   CO|    USA| ALS|\n",
            "+----------+-----+-------+----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+-----+--------+------+-----------+\n",
            "|   date|delay|distance|origin|destination|\n",
            "+-------+-----+--------+------+-----------+\n",
            "|1011245|    6|     602|   ABE|        ATL|\n",
            "|1020600|   -8|     369|   ABE|        DTW|\n",
            "|1021245|   -2|     602|   ABE|        ATL|\n",
            "|1020605|   -4|     602|   ABE|        ATL|\n",
            "|1031245|   -4|     602|   ABE|        ATL|\n",
            "+-------+-----+--------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Создаем временную таблицу\n",
        "foo = (departure\n",
        " .filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and\n",
        " date like '1010%' and delay > 0\"\"\")))\n",
        "\n",
        "foo.createOrReplaceTempView(\"foo\")\n",
        "foo.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP8KxiAJlp5U",
        "outputId": "96ff3e6d-a625-422c-b91a-e1fed1b35198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+------+-----------+\n",
            "|   date|delay|distance|origin|destination|\n",
            "+-------+-----+--------+------+-----------+\n",
            "|1010710|   31|     590|   SEA|        SFO|\n",
            "|1010955|  104|     590|   SEA|        SFO|\n",
            "|1010730|    5|     590|   SEA|        SFO|\n",
            "+-------+-----+--------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Задача 2. Перевести строки в столбцы с помощью Pivot\n",
        "\n"
      ],
      "metadata": {
        "id": "mC1BFgKqD2e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Выведем данные\n",
        "spark.sql(\"\"\"SELECT destination, CAST(SUBSTRING(date, 0, 1) AS int) AS month, delay\n",
        "             FROM departureDelays\n",
        "             WHERE origin = 'SEA'\n",
        "             ORDER BY month desc\"\"\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5J1w5mEddzH",
        "outputId": "2c64c0f6-d076-4c6f-8ccc-a77c21c6bc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+-----+\n",
            "|destination|month|delay|\n",
            "+-----------+-----+-----+\n",
            "|        JFK|    3|   -6|\n",
            "|        ORD|    3|    3|\n",
            "|        DFW|    3|   -2|\n",
            "|        MIA|    3|    0|\n",
            "|        DFW|    3|   -8|\n",
            "+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#В столбце month хранятся месяцы, которые надо перенести в столбцы\n",
        "spark.sql(\"\"\"SELECT *\n",
        "             FROM (SELECT destination, CAST(SUBSTRING(date, 0, 1) AS int) AS month, delay\n",
        "                   FROM departureDelays\n",
        "                   WHERE origin = 'SEA')\n",
        "             PIVOT (CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
        "                    FOR month IN (1 JAN, 2 FEB, 3 MAR))\n",
        "                   \"\"\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWAwa1K7EKVB",
        "outputId": "ce74a44b-3ec6-427f-df27-5f258f01a9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+------------+------------+------------+------------+------------+\n",
            "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|MAR_AvgDelay|MAR_MaxDelay|\n",
            "+-----------+------------+------------+------------+------------+------------+------------+\n",
            "|        GEG|        2.28|          63|        2.87|          60|        4.49|          89|\n",
            "|        BUR|       -2.03|          56|       -1.89|          78|        2.01|         108|\n",
            "|        SNA|       -3.58|          82|       -0.41|          90|       -1.12|         107|\n",
            "|        OAK|       15.82|         385|        8.12|         150|        7.75|         130|\n",
            "|        DCA|       -1.15|          50|        0.07|          34|        5.73|         199|\n",
            "+-----------+------------+------------+------------+------------+------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Построение Lake house на базе pyspark\n",
        "\n",
        "Настройка Apache Spark with Delta Lake\n"
      ],
      "metadata": {
        "id": "5a1XwHIB8pSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Изменим все операции чтения и записи DataFrame - операции для использования формата («дельта») вместо формата («паркет»). Сделаем эти манипуляции с данными о кредите, которые доступны в виде файла Parquet и сохраним их как таблицу Delta Lake.\n",
        "\n",
        "Перед началом выполнения заданий, необходимо убить нынешнюю сессию и установить версию спарка и дельты, которые будут интегрированы с друг другом."
      ],
      "metadata": {
        "id": "50Y8mJjpIdNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/databricks/LearningSparkV2.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH5o1aRxU98J",
        "outputId": "9b14a3b3-1394-43e8-bcaf-890449d1563c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LearningSparkV2'...\n",
            "remote: Enumerating objects: 1720, done.\u001b[K\n",
            "remote: Counting objects: 100% (1720/1720), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1036/1036), done.\u001b[K\n",
            "remote: Total 1720 (delta 546), reused 1691 (delta 541), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1720/1720), 76.97 MiB | 6.86 MiB/s, done.\n",
            "Resolving deltas: 100% (546/546), done.\n",
            "Updating files: 100% (768/768), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.4.1 delta-spark==2.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bFiaqY6Q9KB",
        "outputId": "72e5af7d-39d1-4061-cfc5-042cd25561af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.1\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting delta-spark==2.4.0\n",
            "  Downloading delta_spark-2.4.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.1) (0.10.9.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark==2.4.0) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark==2.4.0) (3.20.2)\n",
            "Downloading delta_spark-2.4.0-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285391 sha256=78f2161b70b4d3b9c0a65e3e96ca0f7e3c593a39426cb7b5a74fa7149bec06e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark, delta-spark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.3\n",
            "    Uninstalling pyspark-3.5.3:\n",
            "      Successfully uninstalled pyspark-3.5.3\n",
            "Successfully installed delta-spark-2.4.0 pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#внесем доп. настройки в конфиг\n",
        "!pyspark --packages io.delta:delta-core_2.12:2.4.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog <<-EOF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGpk5DE3sIAM",
        "outputId": "02c308a8-bd86-4a39-e7ad-12f38d47caf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#создаем папку\n",
        "!mkdir /content/LearningSparkV2/databricks-datasets/learning-spark-v2/loans_delta"
      ],
      "metadata": {
        "id": "iz25obfWcze-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spark.stop()\n",
        "#импорт библиотеки\n",
        "import pyspark\n",
        "from delta import *\n",
        "\n",
        "#открываем сессию\n",
        "bild = pyspark.sql.SparkSession.builder \\\n",
        "    .appName(\"Hive options in PySpark\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "\n",
        "spark = configure_spark_with_delta_pip(bild).getOrCreate()\n",
        "\n",
        "\n",
        "#исходные данные\n",
        "sourcePath = \"/content/LearningSparkV2/databricks-datasets/learning-spark-v2/loans/loan-risks.snappy.parquet\"\n",
        "\n",
        "#ссылка на Delta Lake path\n",
        "deltaPath = \"/content/LearningSparkV2/databricks-datasets/learning-spark-v2/loans_delta\"\n",
        "\n",
        "df = spark.read.format(\"parquet\").load(sourcePath)\n",
        "\n",
        "#Запись в формат дельта\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(deltaPath)\n",
        "#читаем таблицу\n",
        "spark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")"
      ],
      "metadata": {
        "id": "Bm2EqR_h9heY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Теперь к ней можно обратиться как к таблице в реляционной бд\n",
        "spark.sql(\"SELECT * FROM loans_delta WHERE addr_state = 'OR' OR addr_state = 'WA' order by addr_state\").show(500)"
      ],
      "metadata": {
        "id": "Ekc3dObNH4xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Настройка схемы таблицы data lake\n",
        "\n",
        "Перед тем как писать данные в таблицы delta lake, напишем структурную схему таблицы, если файлы которые записываются в таблицу delta lake будут иметь другую схему , то выводится ошибка - чтобы ее не было необходимо добавлять параметр (\"mergeSchema\", \"true\")"
      ],
      "metadata": {
        "id": "Ev5y5MQ8Qmhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#создаем папку\n",
        "!mkdir /content/LearningSparkV2/databricks-datasets/learning-spark-v2/loans_delta_2"
      ],
      "metadata": {
        "id": "x9-0C7BNuo6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n",
        "items = [\n",
        "(1111111, 1000, 1000.0, 'TX', True),\n",
        "(2222222, 2000, 0.0, 'CA', False)\n",
        "]\n",
        "\n",
        "loanUpdates = (spark.createDataFrame(items, cols)\n",
        "                    .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")))\n",
        "\n",
        "#ссылка на Delta Lake path\n",
        "deltaPath_2 = \"/content/LearningSparkV2/databricks-datasets/learning-spark-v2/loans_delta_2\"\n",
        "\n",
        "(loanUpdates.write.format(\"delta\").mode(\"overwrite\")\n",
        " .option(\"mergeSchema\", \"true\")\n",
        " .save(deltaPath_2))\n"
      ],
      "metadata": {
        "id": "p4Am4otpH44o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Преобразование существующих данных в рамках Delta Lake."
      ],
      "metadata": {
        "id": "_7-6YA6ChcmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Необходимо обновить Адрес штата OR на WA"
      ],
      "metadata": {
        "id": "4VC_Zz_Lh2Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\n",
        "\n",
        "#обращение к таблице\n",
        "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
        "deltaTable.update(\"addr_state = 'OR'\", {\"addr_state\": \"'WA'\"})\n"
      ],
      "metadata": {
        "id": "KMot37XGH47J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удалим ячейки где funded_amnt = paid_amnt"
      ],
      "metadata": {
        "id": "nAQcKl75iGWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#обращение к таблице\n",
        "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
        "deltaTable.delete(\"funded_amnt = paid_amnt\")"
      ],
      "metadata": {
        "id": "o5oGVMRsjeY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делаем UPSERT с помощью merge."
      ],
      "metadata": {
        "id": "1PTVs8S5s5IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(deltaTable\n",
        " .alias(\"t\")\n",
        " .merge(loanUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\")\n",
        " .whenMatchedUpdateAll()\n",
        " .whenNotMatchedInsertAll()\n",
        " .execute())"
      ],
      "metadata": {
        "id": "GktdwoPHtVmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделаем аудит изменений таблицы"
      ],
      "metadata": {
        "id": "R6KrgTKS5JYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#deltaTable.history().show(3,False)\n",
        "(deltaTable\n",
        " .history(3)\n",
        " .select(\"version\", \"timestamp\", \"operation\", \"operationParameters\")\n",
        " .show(truncate=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6KOUh-aH49p",
        "outputId": "ef29b7eb-0d28-4a98-f76f-e80c39e992d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|version|timestamp              |operation|operationParameters                                                                                                                                                                     |\n",
            "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|3      |2024-10-24 12:05:55.076|MERGE    |{predicate -> [\"(loan_id#2305L = loan_id#741L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|\n",
            "|2      |2024-10-24 12:05:43.385|DELETE   |{predicate -> [\"(cast(funded_amnt#2306 as double) = paid_amnt#2307)\"]}                                                                                                                  |\n",
            "|1      |2024-10-24 12:05:31.869|UPDATE   |{predicate -> [\"(addr_state#1465 = OR)\"]}                                                                                                                                               |\n",
            "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Версионирование в рамках delta lake\n",
        "(spark.read.format(\"delta\")\n",
        ".option(\"versionAsOf\", \"1\")\n",
        ".load(deltaPath)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EeUx3-UalTn",
        "outputId": "3140eae9-e773-4281-d4b5-18c577e8b9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+---------+----------+\n",
            "|loan_id|funded_amnt|paid_amnt|addr_state|\n",
            "+-------+-----------+---------+----------+\n",
            "|      0|       1000|   182.22|        CA|\n",
            "|      1|       1000|   361.19|        WA|\n",
            "|      2|       1000|   176.26|        TX|\n",
            "|      3|       1000|   1000.0|        OK|\n",
            "|      4|       1000|   249.98|        PA|\n",
            "|      5|       1000|    408.6|        CA|\n",
            "|      6|       1000|   1000.0|        MD|\n",
            "|      7|       1000|   168.81|        OH|\n",
            "|      8|       1000|   193.64|        TX|\n",
            "|      9|       1000|   218.83|        CT|\n",
            "|     10|       1000|   322.37|        NJ|\n",
            "|     11|       1000|   400.61|        NY|\n",
            "|     12|       1000|   1000.0|        FL|\n",
            "|     13|       1000|   165.88|        NJ|\n",
            "|     14|       1000|    190.6|        TX|\n",
            "|     15|       1000|   1000.0|        OH|\n",
            "|     16|       1000|   213.72|        MI|\n",
            "|     17|       1000|   188.89|        MI|\n",
            "|     18|       1000|   237.41|        CA|\n",
            "|     19|       1000|   203.85|        CA|\n",
            "+-------+-----------+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Machine Learning with MLlib"
      ],
      "metadata": {
        "id": "_BEjHK9AL26m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#импорт библиотеки\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#открываем сессию\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Hive options in PySpark\") \\\n",
        "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.0.1\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "Yzx8bV96SEVH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = \"\"\"/content/LearningSparkV2/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet/\"\"\"\n",
        "airbnbDF = spark.read.parquet(filePath)\n",
        "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
        " \"number_of_reviews\", \"price\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWRavKj6L5Fv",
        "outputId": "6b7ffd1b-f465-4251-9f44-5f9998c2c542"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+---------------+--------+---------+-----------------+-----+\n",
            "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
            "+----------------------+---------------+--------+---------+-----------------+-----+\n",
            "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
            "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
            "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
            "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
            "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
            "+----------------------+---------------+--------+---------+-----------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#поделим датафрейм на части\n",
        "#оставим 80% для обучающего набора и 20 для тестового набора\n",
        "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
        "print(f\"\"\"There are {trainDF.count()} rows in the training set,\n",
        "and {testDF.count()} in the test set\"\"\")"
      ],
      "metadata": {
        "id": "D_IJ1zg_Maeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f51306d-d599-4e78-c41d-b88661b85ebc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 5780 rows in the training set,\n",
            "and 1366 in the test set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#построим линейную регрессию для прогнозирования цены с учетом количестова спален\n",
        "#создаем вектор с помощью преобразователя асемблера\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
        "vecTrainDF = vecAssembler.transform(trainDF)\n",
        "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ijZuIfzQX5_",
        "outputId": "6585e8f8-d6b8-44b6-dd6d-e5b3a8eafb41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+-----+\n",
            "|bedrooms|features|price|\n",
            "+--------+--------+-----+\n",
            "|     1.0|   [1.0]|200.0|\n",
            "|     1.0|   [1.0]|130.0|\n",
            "|     1.0|   [1.0]| 95.0|\n",
            "|     1.0|   [1.0]|250.0|\n",
            "|     3.0|   [3.0]|250.0|\n",
            "|     1.0|   [1.0]|115.0|\n",
            "|     1.0|   [1.0]|105.0|\n",
            "|     1.0|   [1.0]| 86.0|\n",
            "|     1.0|   [1.0]|100.0|\n",
            "|     2.0|   [2.0]|220.0|\n",
            "+--------+--------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#После настройки вектора ассемблера готовим и преобразуем данные в формат, который ожидает модель линейной регрессии\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
        "lrModel = lr.fit(vecTrainDF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvik3GZEETCq",
        "outputId": "3b6c4e6f-4baf-476d-e954-2a5028cfb5b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModel: uid=LinearRegression_0d607577432a, numFeatures=1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7HSoB04GBFS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}