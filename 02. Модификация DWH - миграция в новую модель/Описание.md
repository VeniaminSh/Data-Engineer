# Описание проекта ETL

Проект основывается на выгрузке данных из облака яндекса в формате csv и миграция данных в бд postgres.


| Задачи                   | Результаты |
| :-------------------- | :--------------------- |
| Создать витрину для пользователей | Сделал пайплайн в Airflow, который производит миграцию данных из облака яндекса в формате csv в бд postgres в сырьевой слой по API и преобразует их в витрину |  

### Описание пайплайна:

Импорт библитоек:

```
import datetime #время в dag
import time #для ожидания ответа от сервера сон в одной из функций
import psycopg2 #конектор к постгре
 
import requests #выгрузка файлов по api
import json
import pandas as pd #взаимодействие с данными
import numpy as np #манипуляция с данными
from sqlalchemy import create_engine #конектор к постгре
 
#библиотеки для Airflow
from airflow import DAG
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.operators.python import PythonOperator
from airflow.hooks.base import BaseHook
from airflow.models.xcom import XCom
```

Пишем настройки API

```
#обращение к встроенному подключению в airflow
api_conn = BaseHook.get_connection('create_files_api')

#d5dg1j9kt695d30blp03.apigw.yandexcloud.net
api_endpoint = api_conn.host
#5f55e6c0-e9e5-4a9c-b313-63c01fc31460
api_token = api_conn.password

#константы для APi
nickname = "aa-tolmachev"
cohort = "1"
 
headers = {
    "X-API-KEY": api_conn.password,
    "X-Nickname": nickname,
    "X-Cohort": cohort
} 
```

Описываем настройки к постгре
```
#обращение к встроенному подключению в airflow
psql_conn = BaseHook.get_connection('pg_connection')
 
##тестовая инициализация подключения
conn = psycopg2.connect(f"dbname='de' port='{psql_conn.port}' user='{psql_conn.login}' host='{psql_conn.host}' password='{psql_conn.password}'")
cur = conn.cursor()
cur.close()
conn.close()
```


Описание функций:

1. create_files_request

на вход прописываем две константы и параметр ti, благодаря которому можно результат функции передать в другую функцию. Данная функция выводит идентификатор задачи выгрузки, который небоходимо вставить в запрос get для получения ссылок на выгрузку файлов

```
def create_files_request(ti, api_endpoint , headers):
    method_url = '/generate_report'
    r = requests.post('https://'+api_endpoint + method_url, headers=headers)
    response_dict = json.loads(r.content)
    ti.xcom_push(key='task_id', value=response_dict['task_id'])
    print(f"task_id is {response_dict['task_id']}")
    return response_dict['task_id']
```

2. check_report

на вход прописываем две константы и параметр ti, благодаря которому можно результат функции передать в другую функцию. Идентификатор задачи вводим в запрос get и через ~60 сек должны сформироваться ссылки на выгрузку файлов, тк может произойти сбой в работе s3 или любая тех причина, перестрахуемся циклом, когда файлы будут готовы идентификатор status перейдет в режим SUCCESS из режима RANNING цикл прервем

```
def check_report(ti, api_endpoint , headers):
    task_ids = ti.xcom_pull(key='task_id', task_ids=['create_files_request'])
    task_id = task_ids[0]
 
    method_url = '/get_report'
    payload = {'task_id': task_id}

    for i in range(4):
        time.sleep(70)
        r = requests.get('https://' + api_endpoint + method_url, params=payload, headers=headers)
        response_dict = json.loads(r.content)
        print(i, response_dict['status'])
        if response_dict['status'] == 'SUCCESS':
            report_id = response_dict['data']['report_id']
            break

    ti.xcom_push(key='report_id', value=report_id)
    print(f"report_id is {report_id}")
    return report_id
```

3. upload_from_s3_to_pg 

загружаем 3 файла из хранилища s3 в датфреймы, а после в бд postgres в первый слой stage, который является сырьевым (исходные данные)

```
def upload_from_s3_to_pg(ti,nickname,cohort):
    report_ids = ti.xcom_pull(key='report_id', task_ids=['check_ready_report'])
    report_id = report_ids[0]

    #создаем данимичную ссылку 
    storage_url = 'https://storage.yandexcloud.net/s3-sprint3/cohort_{COHORT_NUMBER}/{NICKNAME}/{REPORT_ID}/{FILE_NAME}'
 
    personal_storage_url = storage_url.replace("{COHORT_NUMBER}", cohort)
    personal_storage_url = personal_storage_url.replace("{NICKNAME}", nickname)
    personal_storage_url = personal_storage_url.replace("{REPORT_ID}", report_id)

    #создаем подключение к Постгре для залива таблиц 
    psql_conn = BaseHook.get_connection('pg_connection')
    conn = psycopg2.connect(f"dbname='de' port='{psql_conn.port}' user='{psql_conn.login}' host='{psql_conn.host}' password='{psql_conn.password}'")
    cur = conn.cursor()

    #первая таблица - custom_research
    #читаем csv в датафрейм
    df_customer_research = pd.read_csv(personal_storage_url.replace("{FILE_NAME}", "customer_research.csv") )
    df_customer_research.reset_index(drop = True, inplace = True)
    #создаем копию пустого датафрейма для вставки ddl в постгрес
    df_customer_research_headlines = df_customer_research.copy()
    df_customer_research_headlines = df_customer_research_headlines.iloc[:0]
    engine = create_engine('postgresql+psycopg2://jovyan:jovyan@localhost:5432/de')
    df_customer_research_headlines.to_sql('customer_research', engine, index=False, if_exists='replace', schema='stage')


    #тк кол-во данных может быть большим добавляем насыщаем постгрес инкрементально
    insert_cr = "insert into stage.customer_research (date_id, category_id, geo_id, sales_qty, sales_amt) VALUES {cr_val};"
    i = 0
    step = int(df_customer_research.shape[0] / 100)
    while i <= df_customer_research.shape[0]:
        print('df_customer_research' , i, end='\r')
 
        cr_val =  str([tuple(x) for x in df_customer_research.loc[i:i + step].to_numpy()])[1:-1]
        cur.execute(insert_cr.replace('{cr_val}',cr_val))
        conn.commit()
 
        i += step+1

    #вторая таблица - user_order_log
    #механика такая же, только в инкременте удаляем еще два столбца,тк они не нужны
    df_order_log = pd.read_csv(personal_storage_url.replace("{FILE_NAME}", "user_order_log.csv") )
    df_order_log.reset_index(drop = True, inplace = True)
    df_order_log_headlines = df_order_log[['date_time','city_id','city_name','customer_id','first_name','last_name','item_id','item_name','quantity','payment_amount']].copy()
    df_order_log_headlines = df_order_log_headlines.iloc[:0]
    engine = create_engine('postgresql+psycopg2://jovyan:jovyan@localhost:5432/de')
    df_order_log_headlines.to_sql('user_order_log', engine, index=False, if_exists='replace', schema='stage')
    insert_uol = "insert into stage.user_order_log (date_time, city_id, city_name, customer_id, first_name, last_name, item_id, item_name, quantity, payment_amount) VALUES {uol_val};"
    i = 0
    step = int(df_order_log.shape[0] / 100)
    while i <= df_order_log.shape[0]:
        print('df_order_log',i, end='\r')
 
        uol_val =  str([tuple(x) for x in df_order_log.drop(columns = ['id', 'uniq_id'] , axis = 1).loc[i:i + step].to_numpy()])[1:-1]
        cur.execute(insert_uol.replace('{uol_val}',uol_val))
        conn.commit()
 
 
        i += step+1    

    #третья таблица - user_activity_log
    df_activity_log = pd.read_csv(personal_storage_url.replace("{FILE_NAME}", "user_activity_log.csv") )
    df_activity_log.reset_index(drop = True, inplace = True)
    df_activity_log_headlines = df_activity_log[['date_time', 'action_id','customer_id' ,'quantity']].copy()
    df_activity_log_headlines = df_activity_log_headlines.iloc[:0]
    engine = create_engine('postgresql+psycopg2://jovyan:jovyan@localhost:5432/de')
    df_activity_log_headlines.to_sql('user_activity_log', engine, index=False, if_exists='replace', schema='stage')
    insert_ual = "insert into stage.user_activity_log (date_time, action_id, customer_id, quantity) VALUES {ual_val};"
    i = 0
    step = int(df_activity_log.shape[0] / 100)
    while i <= df_activity_log.shape[0]:
        print('df_activity_log',i, end='\r')
 
        if df_activity_log.loc[i:i + step].shape[0] > 0:
            ual_val =  str([tuple(x) for x in df_activity_log.drop(columns = ['id', 'uniq_id'] , axis = 1).loc[i:i + step].to_numpy()])[1:-1]
            cur.execute(insert_ual.replace('{ual_val}',ual_val))
            conn.commit()
 
 
        i += step+1

    #закрываем открытое подключение к постгрес
    cur.close()
    conn.close()

    return 200 
```
    
4. upload_from_s3_to_pg четвертая функция, мигрируем из слоя stage в витрину для использования

    ```
    #открываем подключение для постгрес
    psql_conn = BaseHook.get_connection('pg_connection')
    conn = psycopg2.connect(f"dbname='de' port='{psql_conn.port}' user='{psql_conn.login}' host='{psql_conn.host}' password='{psql_conn.password}'")
    cur = conn.cursor()

    #сперва очистим заранее заготовленные таблицы для витрины с измерениями, после начнем насыщать их данными 
    cur.execute("""DELETE FROM mart.f_activity;
               DELETE FROM mart.f_daily_sales;
               DELETE FROM mart.d_customer;
               DELETE FROM mart.d_calendar;
               DELETE FROM mart.d_item;
               
               INSERT INTO mart.d_customer(customer_id, first_name, last_name, city_id)
                    WITH qwe AS (
                    select cast(date_time as date) AS dates,
                           customer_id, 
                           city_id, 
                           city_name, 
                           first_name, 
                           last_name,
                           ROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY cast(date_time as date) desc) AS rt
                    from stage.user_order_log 
                    GROUP BY 1,2,3,4,5,6)

                    SELECT DISTINCT customer_id, first_name, last_name, city_id
                    FROM qwe
                    WHERE rt = 1;
                    
                 with all_dates as (
                         select distinct to_date(cast(date_time as TEXT),'YYYY-MM-DD') as date_time from stage.user_activity_log
                              union
                         select distinct to_date(cast(date_time as TEXT),'YYYY-MM-DD') from stage.user_order_log
                              union
                         select distinct to_date(cast(date_id as TEXT),'YYYY-MM-DD') from stage.customer_research
                              order by date_time
                              )

                         INSERT INTO mart.d_calendar(date_id, fact_date, day_num, month_num, month_name, year_num)
                         SELECT DISTINCT 
                                ROW_NUMBER () OVER (ORDER BY date_time) as date_iddate_time,
                                date_time,
                                EXTRACT(DAY FROM date_time) AS day_num,
                                EXTRACT(MONTH FROM date_time) AS month_num,
                                to_char(cast(date_time as timestamp), 'Month') AS month_name,
                                EXTRACT(YEAR FROM date_time) AS year_num
                         FROM all_dates;
                         
                    INSERT INTO mart.d_item(item_id, item_name)
                          SELECT DISTINCT 
                                 item_id,
                                 item_name 
                          FROM stage.user_order_log;""")
    conn.commit() 
    cur.close()
    conn.close()
 
    return 200
    ```


5. update_mart_f_tables аналогично с таблицами фактов

    ```
    def update_mart_f_tables(ti):
        #connection to database
        psql_conn = BaseHook.get_connection('pg_connection')
        conn = psycopg2.connect(f"dbname='de' port='{psql_conn.port}' user='{psql_conn.login}' host='{psql_conn.host}' password='{psql_conn.password}'")
        cur = conn.cursor()
 
    
        cur.execute("INSERT INTO mart.f_activity (activity_id, date_id, click_number)
                SELECT action_id,
                       d.date_id,
                       sum(quantity)
                FROM stage.user_activity_log ual
                       LEFT JOIN mart.d_calendar d 
                       ON to_date(CAST(ual.date_time AS TEXT),'YYYY-MM-DD') = d.fact_date
                GROUP BY 1, 2;
                
                INSERT INTO mart.f_daily_sales (date_id, item_id, customer_id, price, quantity, payment_amount)
                SELECT 
                    date_id,
                    item_id,
                    customer_id,
                    sum(payment_amount) / sum(quantity) AS price ,
                    sum(quantity),
                    sum(payment_amount)
                FROM stage.user_order_log ual
                LEFT JOIN mart.d_calendar d 
                                ON to_date(CAST(ual.date_time AS TEXT),'YYYY-MM-DD') = d.fact_date
                GROUP BY 1,2,3;")
    conn.commit() 
    cur.close()
    conn.close()
 
    return 200
    ```


* Объявляем даг и проставляем иерархию запуска задач

```
#настройки дага
dag = DAG(
    dag_id='EXP_norm',
    schedule_interval='0 0 * * *',
    start_date=datetime.datetime(2024, 5, 17),
    catchup=False,
    dagrun_timeout=datetime.timedelta(minutes=60)
)
 
t_file_request = PythonOperator(task_id='create_files_request',
                                        python_callable=create_files_request,
                                        op_kwargs={'api_endpoint':api_endpoint,
                                                    'headers': headers
                                                    },
                                        dag=dag)
 
t_check_report = PythonOperator(task_id='check_ready_report',
                                        python_callable=check_report,
                                        op_kwargs={'api_endpoint':api_endpoint,
                                                    'headers': headers
                                                    },
                                        dag=dag)
 
t_upload_from_s3_to_pg = PythonOperator(task_id='upload_from_s3_to_pg',
                                        python_callable=upload_from_s3_to_pg,
                                        op_kwargs={'nickname':nickname,
                                                    'cohort': cohort
                                                    },
                                        dag=dag)
 
t_update_mart_d_tables = PythonOperator(task_id='update_mart_d_tables',
                                        python_callable=update_mart_d_tables,
                                        dag=dag)
 
t_update_mart_f_tables = PythonOperator(task_id='update_mart_f_tables',
                                        python_callable=update_mart_f_tables,
                                        dag=dag)
 

#иерархия задач
t_file_request >> t_check_report >> t_upload_from_s3_to_pg >> t_update_mart_d_tables >> t_update_mart_f_tables
```