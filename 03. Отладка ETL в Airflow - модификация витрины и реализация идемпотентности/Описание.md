# Описание отладки проекта ETL, миграция данных + реализация идемпотентности

### Превью проекта:
Проект основывается на выгрузке доработанного инкремента из яндекс обалка по API и отладка уже имеющегося ETL процесса + построение идемпотентности, реализация миграции в рамках постгреса.


| Задачи                   | Результаты |
| :-------------------- | :--------------------- |
| Модификация процессов в пайплане: <br><br> Встроить новый инкремент, который выгружется из яндекс облака по api в ETL. <br> <br> Создать новую витрину данных Retention Rate (возвращаемость клиентов).| Модифицировал пайплайн в Airflow, добавил инкремент с новым полем к уже имеющимся данным. <br><br> Модифицировал имеющиеся витрины под новый инкремент.<br><br> Реализовал новую витрину для расчета возвращаемости клиентов - период неделя от вчерашнего дня. Поднял идемпотентность. |

### Описание пайплайна:

Импорт библиотек:

```import time
import requests
import json
import pandas as pd
import logging             # <-для логирования кода

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator, BranchPythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.hooks.http_hook import HttpHook
```

Описание подключений в рамках коннекторов airflow:


```
# Описание настроек
# Connection Id - http_conn_id
# Connection Type - HTTP
# Host - https://d5dg1j9kt695d30blp03.apigw.yandexcloud.net
# Extra - {"api_key": "5f55e6c0-e9e5-4a9c-b313-63c01fc31460", "nickname": "Nick", "cohort": "1"}
http_conn_id = HttpHook.get_connection('http_conn_id')

api_key = http_conn_id.extra_dejson.get('api_key')
base_url = http_conn_id.host
nickname = http_conn_id.extra_dejson.get('nickname')
cohort = http_conn_id.extra_dejson.get('cohort')

# Описание настроек - к это подключению будем обращаться через PostgresHook
# Connection Id - postgresql_de
# Connection Type - Postgres
# Host - localhost
# Schema - de
# Login - jovyan
# Port - 5432
postgres_conn_id = 'postgresql_de'
```

Headers - настройки для генерации инкремента 
```
headers = {
    'X-Nickname': nickname,
    'X-Cohort': cohort,
    'X-Project': 'True',
    'X-API-KEY': api_key,
    'Content-Type': 'application/x-www-form-urlencoded'
}
```

Создаю предметную область логирования для бесперебойной работы и быстрой отладки 
```
logging.basicConfig(level=logging.INFO, filename='py_log.log',filemode="w", format='%(asctime)s - %(levelname)s - %(message)s')
```

Описание функций, которые будут вызваны в dag для выполнения поставленых задач:
```
#Генерация отчета методом post для получения ключа к методу get

def generate_report(ti): #параметр ti передает инфу из одной функции в другую
    logging.info('generate_report - Making request') #запись лога
    response = requests.post(f'{base_url}/generate_report', headers=headers) #запрос к серверу
    response.raise_for_status() #предупреждение если ответ отрицательный
    task_id = json.loads(response.content)['task_id'] #ключ для метода get
    ti.xcom_push(key='task_id', value=task_id) #передача параметра
    logging.info(f'generate_report - Response is {response.content}') #логирование
    return task_id
```

```
#Генерация отчета методом get для получения ключа к инкременту

def get_report(ti):
    task_id = ti.xcom_pull(key='task_id')
    report_id = None
    
    #цикл проходит до момента как статус ранинг изменится на саксес
    for i in range(20):
        response = requests.get(f'{base_url}/get_report?task_id={task_id}', headers = headers)
        response.raise_for_status()
        task_id= json.loads(response.content)['task_id']
        status = json.loads(response.content)['status']
        if status == 'SUCCESS':
            logging.info(f'generate_report - status SUCCESS')
            report_id = json.loads(response.content)['data']['report_id']
            break
        else:
            time.sleep(10)
        
    if not report_id:
        raise TimeoutError()
        
    ti.xcom_push(key='report_id', value=report_id)
```

```
#Генерация ключа для инкремента по дате

def get_increment(ti, date): # в параметры передается ti и дата, за которую необходим инкремент
    logging.info(f'get_increment - Start') #логирование
    report_id = ti.xcom_pull(key='report_id')
    response = requests.get(f'{base_url}/get_increment?report_id={report_id}&date={str(date)}T00:00:00', 
                            headers = headers) #get запрос на выгрузку ключа для инкремента по дате
    response.raise_for_status()
    increment_id = json.loads(response.content)['data']['increment_id']
    if not increment_id: #если инкремента нет, выводи ошибку
        logging.info(f'generate_report - not increment_id')
        raise ValueError(f'Increment is empty. Most probably due to error in API call.')
    logging.info(f'generate_report - increment_id is ready')
    ti.xcom_push(key='increment_id', value=increment_id)
```

```
#Выгрузка и предобработка перед вставкой

def upload_data_to_staging(filename, date, pg_table, pg_schema, ti): #функция получает на вход название файла, дату, таблицу и схему в постгресе
    logging.info(f'upload_data_to_staging - start')
    increment_id = ti.xcom_pull(key='increment_id')
    s3_filename = f'https://storage.yandexcloud.net/s3-sprint3/cohort_{cohort}/{nickname}/project/{increment_id}/{filename}' #генерация ссылки, по которой доступен инкремент
    logging.info(f'upload_data_to_staging = {s3_filename}')
    local_filename = date.replace('-','') + '_' + filename # формируем название файла, куда будет помещен инкремент
    logging.info(f'upload_data_to_staging = {local_filename}')
    response = requests.get(s3_filename) #достаем инкремент
    response.raise_for_status()
    open(f"{local_filename}", "wb").write(response.content) #окрываем файл и записаным в него инкрементом
    logging.info(f'upload_data_to_staging = local file is write')

    df = pd.read_csv(local_filename) #перевод файла в формат csv
    df = df.drop('id', axis = 1)
    df= df.drop_duplicates(subset=['uniq_id'])
    
    # т.к. первый инкремент приходит в старом варианте (все товары считаются проданнми и имеют статус shipped), без столбца status, необходимо добавить в него столбец,для формирования общей структуры таблицы
    if 'status' not in df.columns: 
        df['status'] = 'shipped'
    
    #используем подключение к постгресу
    postgres_hook= PostgresHook(postgres_conn_id)
    
    #вытащим движок, на основе которого будет пуш в постгрес
    engine = postgres_hook.get_sqlalchemy_engine()

    #т.к. в прошлой выгрузке не было статуса, его необходимо добавить
    add_column = f'alter table {pg_schema}.{pg_table} add column if not exists status VARCHAR'
    #пуш в постгрес
    engine.execute(add_column)
    logging.info(f'upload_data_to_staging - columns status add in {pg_schema}.{pg_table}')
    
    #применим свойство идемпотентности - удалим в таблице постгреса данные, которые будем загружать 
    trunc_table= f"delete from {pg_schema}.{pg_table} where date_time::date = '{date}'"
    engine.execute(trunc_table)
    
    #загрузим в постгрес данные методом append, т.е. данные добавляются накоплением
    add_in_table= df.to_sql(pg_table, engine, schema=pg_schema, if_exists='append', index=False)
    logging.info(f'upload_data_to_staging - in {pg_schema}.{pg_table} added {df.shape[0]} rows')
```

Опишем настройки дага
```
args = {
    "owner" : "Veniamin",
    'email': ['v.shendogan@gmail.com'],
    'email_on_failure': True, #отправка письма на почту при ошибке
    'email_on_retry': False,
    'retries': 2
}
```

Параметр даты в airflow, дата запуска дага.
```
business_dt = '{{ ds }}'
```

Описание дага в airflow
```
with DAG( 
    'sales',
    default_args=args,
    catchup=True, #разрешает заупск за прошлые периоды
    start_date=datetime.today() - timedelta(days=7), #даг будет запускаться за 7 дней от нынешней даты
    end_date=datetime.today() - timedelta(days=1) #даг завершит запускаться за один день от нынешней даты, по дефолту интервал запуска один раз в день, так и нужно, не указываем этот параметр
) as dag:
    #запускаем генерацию отчета методом post
    f_generate_report = PythonOperator(
        task_id='generate_report',
        python_callable=generate_report)

    #запускаем генерацию отчета методом get
    f_get_report = PythonOperator(
        task_id='get_report',
        python_callable=get_report)
    
    #запуск генерации ключа для инкремента
    f_get_increment = PythonOperator(
        task_id='get_increment',
        python_callable=get_increment,
        op_kwargs={'date': business_dt}) #дата запуска дага, первый инкремент - 7 дней от нынешней даты
    
    #запуск функции выгрузки и преобразования инкремента из облака
    f_upload_data_to_staging = PythonOperator(
        task_id='upload_data_to_staging',
        python_callable=upload_data_to_staging,
        op_kwargs={'date': business_dt, #дата запуска дага 
                    'filename':'user_order_log_inc.csv', #название файла
                    'pg_table':'user_order_log', #название таблицы в постгресе
                    'pg_schema':'staging'}) #название схемы в постгресе
```

<br>
</br>

[f_item](https://github.com/VeniaminSh/Data-Engineer/blob/master/03.%20%D0%9E%D1%82%D0%BB%D0%B0%D0%B4%D0%BA%D0%B0%20ETL%20%D0%B2%20Airflow%20-%20%D0%BC%D0%BE%D0%B4%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B2%D0%B8%D1%82%D1%80%D0%B8%D0%BD%D1%8B%20%D0%B8%20%D1%80%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%D0%B4%D0%B5%D0%BC%D0%BF%D0%BE%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8/scr/d_item.sql) <-- таблица лежит здесь
```   
    #заполнение таблицы в постгре 
    f_item_table = PostgresOperator(
        task_id='f_item',
        postgres_conn_id=postgres_conn_id,
        sql="sql/mart.d_item.sql")
```

<br>
</br>

[f_customer](https://github.com/VeniaminSh/Data-Engineer/blob/master/03.%20%D0%9E%D1%82%D0%BB%D0%B0%D0%B4%D0%BA%D0%B0%20ETL%20%D0%B2%20Airflow%20-%20%D0%BC%D0%BE%D0%B4%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B2%D0%B8%D1%82%D1%80%D0%B8%D0%BD%D1%8B%20%D0%B8%20%D1%80%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%D0%B4%D0%B5%D0%BC%D0%BF%D0%BE%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8/scr/d_customer.sql) <-- таблица лежит здесь
```   
    #заполнение таблицы в постгре 
    f_customer_table = PostgresOperator(
        task_id='f_customer',
        postgres_conn_id=postgres_conn_id,
        sql="sql/mart.d_customer.sql")
```
<br>
</br>

[f_city](https://github.com/VeniaminSh/Data-Engineer/blob/master/03.%20%D0%9E%D1%82%D0%BB%D0%B0%D0%B4%D0%BA%D0%B0%20ETL%20%D0%B2%20Airflow%20-%20%D0%BC%D0%BE%D0%B4%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B2%D0%B8%D1%82%D1%80%D0%B8%D0%BD%D1%8B%20%D0%B8%20%D1%80%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%D0%B4%D0%B5%D0%BC%D0%BF%D0%BE%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8/scr/d_city.sql) <-- таблица лежит здесь
``` 
    f_city_table = PostgresOperator(
        task_id='f_city',
        postgres_conn_id=postgres_conn_id,
        sql="sql/mart.d_city.sql")
``` 
<br>
</br>

[f_sales](https://github.com/VeniaminSh/Data-Engineer/blob/master/03.%20%D0%9E%D1%82%D0%BB%D0%B0%D0%B4%D0%BA%D0%B0%20ETL%20%D0%B2%20Airflow%20-%20%D0%BC%D0%BE%D0%B4%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B2%D0%B8%D1%82%D1%80%D0%B8%D0%BD%D1%8B%20%D0%B8%20%D1%80%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%D0%B4%D0%B5%D0%BC%D0%BF%D0%BE%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8/scr/f_sales.sql) <-- таблица лежит здесь
```
    f_sales = PostgresOperator(
        task_id='f_sales',
        postgres_conn_id=postgres_conn_id,
        sql="sql/mart.f_sales.sql",
        parameters={"date":{business_dt}}
    )
``` 
<br>
</br>
Витрина для расчета возвращаемости клиентов
<br>
</br>

> <details>
> <summary><span style="font-size: 14pt;">(clik) ТЗ к витрине:</span> </summary>
> <br> </br>
>
> Эта витрина должна отражать следующую информацию:
> * Рассматриваемый период — weekly.
> * Возвращаемость клиентов:
>    * new — кол-во клиентов, которые оформили один заказ за рассматриваемый период;
>   * returning — кол-во клиентов, которые оформили более одного заказа за рассматриваемый период;
>   * refunded — кол-во клиентов, которые вернули заказ за рассматриваемый период.
> * Доход (revenue) и refunded для каждой категории покупателей.
>
> <br> </br>
> **Схема витрины
> mart.f_customer_retention**
> 1. new_customers_count — кол-во новых клиентов (тех, которые сделали только один 
заказ за рассматриваемый промежуток времени).
> 2. returning_customers_count — кол-во вернувшихся клиентов (тех,
которые сделали только несколько заказов за рассматриваемый промежуток времени).
> 3. refunded_customer_count — кол-во клиентов, оформивших возврат за 
рассматриваемый промежуток времени.
> 4. period_name — weekly.
> 5. period_id — идентификатор периода (номер недели или номер месяца).
> 6. item_id — идентификатор категории товара.
> 7. new_customers_revenue — доход с новых клиентов.
> 8. returning_customers_revenue — доход с вернувшихся клиентов.
> 9. customers_refunded — количество возвратов клиентов. 
> <br> </br>
>
>

> </details>
<br>
</br>

[f_customer_retention](https://github.com/VeniaminSh/Data-Engineer/blob/master/03.%20%D0%9E%D1%82%D0%BB%D0%B0%D0%B4%D0%BA%D0%B0%20ETL%20%D0%B2%20Airflow%20-%20%D0%BC%D0%BE%D0%B4%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B2%D0%B8%D1%82%D1%80%D0%B8%D0%BD%D1%8B%20%D0%B8%20%D1%80%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%D0%B4%D0%B5%D0%BC%D0%BF%D0%BE%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8/scr/d_customer_retention.sql) <-- таблица лежит здесь
```
    f_customer_retention = PostgresOperator(
        task_id='f_customer_retention',
        postgres_conn_id=postgres_conn_id,
        sql="sql/mart.d_customer_retention.sql",
        parameters={"date":{business_dt}}
    )
```

### Итого реализовано: 
1. Для трех таблиц (f_sales, f_customer_retention, user_order_log) реализована идемподентность, т.е. при повторных запусках даг, данные в них не будут дублироваться.
2. Модификация таблицы f_sales, строки мер с статусом refunded выведены со знаком -, так как этот статус не учитывается в графике маркетологов.
3. Вывод инкремента за неделю от сегодняшнего дня, модификация пайплана под поля инкремента и добавление его в постгрес.
4. Реализация витрины f_customer_retention для расчета возвращаемости клиентов.
